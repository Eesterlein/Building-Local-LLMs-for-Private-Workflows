# Building-Local-LLMs-for-Private-Workflows
This repo showcases two fully offline, privacy-first AI projects built using **Ollama**, **LlamaIndex**, and open-source LLMs on a MacBook Pro M1 Max.
# ğŸ¤– Building Local LLMs for Private Workflows

This repo showcases two fully offline, privacy-first AI projects built using **Ollama**, **LlamaIndex**, and open-source LLMs on a MacBook Pro M1 Max.

---

## ğŸš€ Project 1: Private Coding Copilot

A fast, private coding assistant that runs entirely on-device using:

- ğŸ§  `DeepSeek-Coder 6.7B` via Ollama
- âŒ¨ï¸ Prompted directly through Terminal
- âš¡ No internet connection or cloud API required

**Sample Prompts:**
- â€œWrite a Python function that checks if a number is primeâ€
- â€œCan you help me debug this error?â€
- â€œTranslate this Python code into JavaScriptâ€


---

## ğŸ—‚ï¸ Project 2: Internal Company Policy Chatbot

A Retrieval-Augmented Generation (RAG) chatbot that answers questions based on internal policy docs using:

- ğŸ§  Ollama + Mistral for local LLM + embeddings
- ğŸ“š LlamaIndex for document search
- ğŸ’» Python CLI for user interaction

**Test Prompts:**
- â€œWhat is the maximum PTO carryover?â€
- â€œWhat happens if I lose a company laptop?â€
- â€œHow do I report a safety hazard?â€

ğŸ“„ Sample document: https://doc.clickup.com/9013904302/d/h/8cmagxe-53/ae1ae01503eecf7

---

## ğŸ“ Resources

- [ğŸ§¾ Full project write-up: Click here](Building%20Local%20LLMs.pdf)
- ğŸ§ª View-only notebook on Kaggle: [Kaggle: Building Local LLMs](https://www.kaggle.com/code/elissaesterlein/building-local-llms-for-private-workflows)

---

âš ï¸ These projects are designed to run locally. Kaggle or cloud environments do not support Ollama or local model inference.

---

## ğŸ”§ Requirements

```txt
ollama
llama-index
llama-index-llms-ollama
llama-index-embeddings-ollama
pypdf
